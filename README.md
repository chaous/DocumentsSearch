# отчет: Проектирование ML-системы — Задание 10
### Курс Проектирование систем машинного обучения
### Студент: Илья Рожков Алексеевич
### Группа: МЛ-2025

# Введение и постановка задачи
Проектируется система для семантического поиска по внутренним корпоративным документам крупной организации.
Система должна обеспечивать быстрый и точный поиск по отчётам, презентациям, внутренним вики-страницам и другим источникам знаний, используя векторный поиск на основе заранее рассчитанных эмбеддингов документов.
Обработка запроса пользователя должна выполняться без обращения к тяжёлым генеративным моделям в режиме онлайн: используются компактные модели для получения эмбеддингов запросов и оптимизированный векторный индекс. Поиск должен учитывать сложную модель прав доступа, чтобы каждый пользователь видел только те документы, к которым у него есть разрешение.
Система должна работать под высокой нагрузкой и масштабироваться под большое количество пользователей в корпорации.



# Бизнес-цели
- 1. **Почти мгновенный доступ к информации**
Сократить среднее время поиска внутренних документов (регламентов, отчётов, презентаций, вики-страниц) с нескольких минут до **долей секунды** благодаря высокопроизводительному семантическому поиску с задержкой не более **417 мс**.
Это **повышает скорость принятия решений** и **снижает операционные задержки** в работе сотрудников.
- 2. **Улучшение релевантности результатов поиска**
Повысить точность и полезность выдачи за счёт использования мл поиска, чтобы сотрудники находили **нужные документы с первого запроса**.
Цель — уменьшить количество повторных запросов и повысить эффективность работы.
- 3. **Поддержка большого числа пользователей без деградации качества**
Обеспечить стабильную работу системы для **1,288,472 DAU** и **пиковых 16,994 RPS**, сохраняя высокую скорость и релевантность поиска.
Это снижает нагрузку на внутренние службы поддержки и повышает вовлечённость пользователей в использование корпоративного хранилища знаний.


**Требования к системе:**
- Задержка (latency) ответа не должна превышать **417 мс**.
- Система должна обслуживать **1,288,472 активных пользователей в день (DAU)**, с пиковой нагрузкой в **16,994 запросов в секунду (RPS)**.
- Система должна быть масштабируемой и отказоустойчивой.



# Часть 1: Формулировка ML-задачи и выбор модели
- **1. Определение ML-задачи**
Задачу можно сформулировать как задачу ранжирования документов (Learning-to-Rank) в рамках семантического поиска.
Для каждого пользовательского запроса система должна упорядочить набор доступных пользователю документов по степени их релевантности.
Поиск выполняется с учётом семантического сходства между запросом и документами, а также ограничений доступа (пользователь может видеть только разрешённые документы).


## Входные данные:
Текстовый запрос пользователя на естественном языке.
Корпус внутренних документов (отчёты, презентации, вики), представленных в виде текстовых фрагментов.
Метаданные документов:
идентификаторы проектов и команд;
информация о правах доступа (ACL / RBAC);
тип документа, дата создания и версия.
Выходные данные:
Упорядоченный список документов (или фрагментов документов), релевантных запросу пользователя.
Для каждого документа — числовой скор релевантности.


## Целевая переменная:
Целевой переменной является оценка релевантности документа запросу.
В явном виде она может отсутствовать, поэтому используется одна из следующих форм:
- Неявная обратная связь пользователей:
- клики по результатам поиска;
- время просмотра документа;
- повторные запросы.
## Слабая разметка:
- бинарная метка релевантности (релевантен / нерелевантен);
- порядковая оценка (например, 0 — нерелевантен, 1 — частично релевантен, 2 — релевантен).
## Прокси-таргет:
- Семантическое сходство между эмбеддингом запроса и эмбеддингом документа, используемое как обучающий сигнал.
- Таким образом, задача не предполагает существование «идеального ответа», а оптимизируется по метрикам качества ранжирования (Precision@k, recal@K).



# 2. Выбор модели
Рассмотрим два подхода к построению системы семантического поиска по внутренним корпоративным документам.
## Подход 1: Прямой семантический поиск по эмбеддингам (Vector Similarity Search)
В данном подходе каждый документ и пользовательский запрос представляются в виде векторных эмбеддингов, полученных с помощью компактной нейросетевой модели.
Поиск осуществляется путём вычисления близости между эмбеддингом запроса и эмбеддингами документов с использованием приближённого поиска ближайших соседей (Approximate Nearest Neighbors).
### Преимущества:
- Очень высокая скорость поиска, что позволяет уложиться в ограничение по задержке 417 мс.
- Простая и хорошо масштабируемая архитектура.
- Возможность предварительного вычисления эмбеддингов документов оффлайн.
- Минимальные вычислительные затраты в режиме онлайн.
### Недостатки:
- Релевантность определяется только локальным сходством запроса и документа.
- Не учитываются глобальные связи между документами (темы, контекст, совместное использование).
- Сложно улучшать качество без увеличения размерности эмбеддингов или сложности модели.
## Подход 2: Семантический поиск с использованием графа документов на основе эмбеддингов
В данном подходе документы представляются в виде вершин графа, а рёбра формируются оффлайн на основе семантической близости их эмбеддингов (например, по k-ближайшим соседям).
Граф документов строится заранее на этапе подготовки данных и отражает семантические связи между документами.
В процессе поиска выполняется быстрый векторный retrieval для получения начального набора кандидатов, после чего используется навигация по предварительно построенному графу документов для уточнения и дополнения списка релевантных результатов.
### Преимущества:
- Более высокая релевантность за счёт учёта глобальной структуры корпуса документов.
- Возможность находить связанные документы, которые не являются ближайшими по векторному сходству.
- Улучшение качества поиска без использования тяжёлых моделей в режиме онлайн.
- Возможность объяснять результаты поиска через связи в графе.
- Навигация по графу позволяет ограничить пространство поиска и сократить количество проверяемых документов по сравнению с полным перебором.
### Недостатки:
- Более сложная оффлайн-подготовка данных (построение и обновление графа).
- Дополнительные затраты памяти на хранение графовой структуры.
- Усложнение логики поиска по сравнению с прямым ANN.
## Выбор модели
Для данной задачи выбирается семантический поиск с использованием графа документов, построенного на основе эмбеддингов.
Несмотря на более сложную подготовку данных, данный подход позволяет существенно повысить релевантность результатов поиска, сохранив при этом высокую скорость работы.
Отсутствие тяжёлых моделей в режиме онлайн и возможность параллельной обработки запросов позволяют надёжно уложиться в требование по задержке 417 мс и обеспечить масштабируемость системы при высокой нагрузке.

# Часть 2: Проектирование архитектуры

## 1. Высокоуровневая архитектура системы

Высокоуровневая архитектура показывает взаимодействие основных компонентов
системы внутреннего семантического поиска: от сбора и обработки документов
до развертывания поискового сервиса и мониторинга качества и
производительности. Система должна обеспечивать быстрый поиск (не более
417 мс) при высокой нагрузке и строго соблюдать контроль доступа
к документам.

Ключевые компоненты:

- **Document Sources**:
Корпоративные источники данных: файловые хранилища
(SharePoint, сетевые диски), корпоративная вики (Confluence),
базы отчетов и презентаций, внутренние порталы.

- **Ingestion Service (Connectors)**:
Набор сервисов для регулярного извлечения документов и метаданных
(версия, автор, дата, проект, ссылки, права доступа).

- **Preprocessing & Chunking**:
Сервис предобработки документов, включающий извлечение текста из
PDF, DOCX, PPTX и HTML, очистку данных и разбиение документов
на фрагменты (чанки) для повышения точности поиска.

- **Embedding Service (Offline):**
Сервис вычисления эмбеддингов документов и чанков в режиме оффлайн
по расписанию или при обновлении данных.

- **Query Embedding Service (Online):**
Лёгкий сервис (или модуль внутри Search API), вычисляющий эмбеддинг
пользовательского запроса в реальном времени. Оптимизируется под latency
(батчинг, кэширование, ONNX/TensorRT при необходимости).


- **Vector Index (ANN Store):**
Векторное хранилище с поддержкой приближенного поиска ближайших
соседей (ANN), обеспечивающее быстрый retrieval кандидатов
по эмбеддингу запроса.

- **Document Graph Store:**
Хранилище графа документов, построенного оффлайн на основе
семантической близости эмбеддингов (k-ближайших соседей).
Используется для навигации и расширения результатов поиска.

- **Access Control Service (ACL/RBAC):**
Сервис контроля доступа, интегрированный с корпоративной системой
аутентификации и авторизации (SSO / IAM). Обеспечивает фильтрацию
документов в соответствии с правами пользователя.

- **Search API (Inference / Serving):**
Отдельный сервис-оркестратор, принимающий поисковые запросы от пользовательского
интерфейса через API Gateway и координирующий выполнение онлайн-поиска.
Search API отвечает за:
- получение и валидацию текстового запроса пользователя;
- вызов сервиса эмбеддинга запроса (Query Embedding Service);
- первичный retrieval кандидатов из Vector Index (ANN);
- навигацию и расширение результатов с использованием графа документов;
- фильтрацию документов по правам доступа (ACL/RBAC);
- финальное ранжирование и формирование ответа пользователю.
- деградационные режимы (fallback): 
  при недоступности графа поиск выполняется только по ANN.


Сервис развёртывается в Kubernetes и масштабируется горизонтально для
обслуживания пиковых **16 994 RPS** при целевой задержке ответа
не более **417 мс**.


- **Metadata Store:**
База данных для хранения метаданных документов
(названия, ссылки, версии, владельцы, теги).

- **User Interface:**
Корпоративный веб-интерфейс поиска, через который пользователи
выполняют запросы и получают результаты поиска.

- **Monitoring & Logging:**
Система мониторинга и логирования для отслеживания
производительности (latency, RPS, ошибки), качества поиска
и аудита доступа.
Инструменты: Prometheus, Grafana, централизованное логирование.

- **Training & Evaluation Pipeline:**
Конвейер обучения и оценки моделей эмбеддингов и параметров поиска
на основе логов пользовательского поведения и обратной связи. (не первая итерация)

![Высокоуровневая архитектура системы](high_level_architecture_v2.png)


## Архитектура Data Pipeline

Data Pipeline описывает процесс сбора и подготовки данных для системы
семантического поиска по корпоративным документам. Пайплайн выполняется
оффлайн и обеспечивает обновление индексов и графа документов.

Этапы:

1. **Сбор данных:**
Коннекторы извлекают документы из корпоративных источников
(SharePoint/файловые хранилища, Wiki/Confluence, порталы), а также
метаданные и права доступа (ACL/RBAC).
Для документов с высоким приоритетом (например, регламенты безопасности,
оперативные инструкции, срочные объявления) используется событийное
обновление: при изменении документа автоматически запускается
переиндексация и обновление эмбеддингов.
Для остальных документов применяется периодическое обновление по
расписанию (batch-обработка), что позволяет снизить нагрузку на систему
и оптимизировать использование ресурсов.

2. **Архивирование:**
Сырые документы сохраняются в централизованное хранилище
(например, S3/HDFS/объектное хранилище) для аудита и повторной обработки.

3. **Предобработка:**
Извлекается текст из PDF/DOCX/PPTX/HTML, выполняется очистка и разбиение
на фрагменты (чанки). Параллельно извлекаются изображения и подписи
(если присутствуют: диаграммы, схемы, скриншоты).

4. **Построение эмбеддингов:**
Для текстовых чанков оффлайн вычисляются эмбеддинги с помощью
предобученной Transformer-модели (например, BERT-подобной модели для
sentence embeddings).
Для изображений (если они важны) вычисляются визуальные эмбеддинги
(например, CLIP/Vision Transformer), после чего они объединяются с
текстовыми эмбеддингами (раннее или позднее объединение) и сохраняются
как мультимодальные представления документа/чанка.

5. **Индексация:**
Эмбеддинги добавляются в векторный индекс (ANN), который обеспечивает
быстрый retrieval top-K кандидатов по запросу.

6. **Построение графа документов:**
На основе эмбеддингов оффлайн формируется граф документов/чанков,
где рёбра отражают семантическую близость (например, k-ближайших
соседей). Граф используется для расширения и уточнения результатов поиска.

7. Хранилища:
- **Vector Store**: хранение эмбеддингов документов и чанков, а также
  приближённого индекса (ANN) для быстрого поиска top-K кандидатов.
- **Graph Store**: хранение графа документов, построенного на основе
  семантической близости, включая связи между документами, а также
  основные метаданные (заголовки, ссылки, теги) и информацию о доступе (ACL).


### Примечание:
На старте обучение моделей не выполняется — используются предобученные
модели эмбеддингов. <!--Пайплайн регулярно обновляет индекс и граф при
появлении новых документов или изменении прав доступа. -->

![Архитектура Data Pipeline](data_pipeline.png)


## Архитектура Inference Pipeline (Serving)

Inference Pipeline описывает онлайн-контур обработки пользовательских запросов
к системе внутреннего семантического поиска. Контур ориентирован на высокую
пропускную способность и малую задержку ответа (не более **417 мс**) при
пиковой нагрузке до **16 994 RPS**, а также на строгий контроль доступа к данным.

Компоненты:

- **API Gateway / Load Balancer:**
  Принимает поисковые запросы от пользовательского интерфейса (веб-портал),
  выполняет аутентификацию, TLS termination, rate limiting и распределяет нагрузку
  между репликами Search API.

- **Search API (Inference Service) в Kubernetes:**
  Основной сервис обработки запроса:
  1) нормализация и парсинг запроса,
  2) вычисление эмбеддинга запроса,
  3) быстрый ANN-retrieval top-K кандидатов из векторного индекса,
  4) расширение кандидатов с использованием графа документов (опционально),
  5) фильтрация по правам доступа (ACL/RBAC),
  6) финальное ранжирование и формирование ответа (сниппеты, ссылки, метаданные).

- **Query Embedding Runtime:**
  Сервис вычисления эмбеддинга запроса на основе предобученной
  Transformer-модели (BERT-подобной). Для снижения задержек применяются
  оптимизации (батчинг, кэширование, ONNX/TensorRT при необходимости).
  Обучение моделей в онлайн-контуре не выполняется.

- **Vector Index (ANN Store):**
  Векторный индекс для быстрого поиска ближайших эмбеддингов документов/чанков.
  Индекс шардируется и реплицируется для обеспечения масштабируемости и
  отказоустойчивости при высоком RPS.

- **Document Graph Store (k-NN граф):**
  Хранилище семантического графа документов, используемого для расширения
  и уточнения результатов поиска, повышения полноты и устойчивости выдачи.

- **Document Graph Store (k-NN граф):**
  Хранит семантические связи между документами, а также основные метаданные
  (заголовки, ссылки, теги) и информацию о доступе (ACL)

- **Access Control Service (SSO / IAM, ACL / RBAC):**
  Сервис проверки прав доступа пользователя. Гарантирует, что в выдачу
  попадают только документы, доступные конкретному пользователю или группе.
  Поддерживается аудит обращений к защищённым данным.

- **Redis Cache:**
  Кэширует популярные запросы и промежуточные результаты (эмбеддинги запроса,
  результаты ANN-retrieval) для снижения нагрузки и уменьшения latency.
  Используется короткий TTL и инвалидация при изменении документов или прав доступа.

- **HPA:**
  Автоматическое масштабирование реплик Search API и сервиса эмбеддингов
  на основе метрик CPU/GPU, RPS и p95/p99 latency.

- **Эксперименты: A/B-тесты и многоуровневые бандиты**
  
  - **A/B-тестирование (для важных изменений):**
    Используется для изменений, которые потенциально влияют на ключевые свойства
    системы (релевантность поиска, корректность фильтрации по ACL/RBAC, стабильность
    и задержку). Примеры: новая модель эмбеддингов, изменение схемы retrieval
    (ANN параметров), новая логика ранжирования, изменения в фильтрации по правам.
    Запуск осуществляется через строго контролируемые эксперименты (A/B) с
    фиксированным разбиением трафика и проверкой guardrail-метрик:
    p95/p99 latency, error rate, доля пустых результатов, жалобы пользователей,
    показатели вовлечённости (CTR/дальнейшие клики), а также аудит корректности ACL.

  - **Многоуровневые бандиты / Contextual Bandits (для не критичных улучшений):**
    Применяются для локальных UX- и UI-улучшений, которые не должны влиять на
    безопасность и базовую корректность выдачи. Примеры: порядок отображения блоков,
    формат сниппета, подсветка, варианты автодополнения, лёгкие эвристики
    переранжирования в пределах уже разрешённого набора результатов.
    Бандиты позволяют автоматически выбирать лучший вариант на основе
    пользовательской обратной связи (например, CTR, dwell time, успешное открытие
    документа) и технических метрик (например, latency/cost), ускоряя оптимизацию
    без необходимости длительных фиксированных A/B для каждой мелкой правки.


- **Monitoring & Logging (Prometheus / Grafana / Log Storage):**
  Сбор метрик (latency p50/p95/p99, RPS, error rate, cache hit rate),
  централизованное логирование и алертинг при деградации качества сервиса.


![Архитектура Inference Pipeline](inference_pipeline.png)


## Часть 3: Расчёты и нефункциональные требования

В данном разделе приведены инженерные оценки требований к хранилищу,
пропускной способности и вычислительным ресурсам системы внутреннего
семантического поиска с учётом следующих ограничений:

- максимальная задержка ответа: ≤ 417 мс;
- пиковая нагрузка: 16 994 запросов в секунду (RPS);
- масштабируемость и отказоустойчивость.

---

### 3.1 Расчёт требований к хранилищу

#### Сырые документы (Raw Storage)

Допущения для крупной корпорации:

- средний размер одного документа (PDF/DOCX/PPTX/HTML): 2 МБ;
- количество новых и обновлённых документов: 2 000 000 в день;
- срок хранения (retention): 180 дней.

Объём данных в день:

2 000 000 документов × 2 МБ = 4 000 000 МБ ≈ 4 ТБ/день

Общий объём за 180 дней:

4 ТБ/день × 180 дней = 720 ТБ

---

#### Эмбеддинги документов

Допущения:

- среднее число чанков на документ: 20;
- размерность эмбеддинга: 768;
- формат хранения: FP16 (2 байта).

Размер одного эмбеддинга:

768 × 2 байта = 1536 байт ≈ 1.5 КБ

Количество чанков в день:

2 000 000 документов × 20 чанков = 40 000 000 чанков/день

Объём эмбеддингов в день:

40 000 000 × 1.5 КБ ≈ 60 ГБ/день

Объём за 180 дней:

60 ГБ × 180 ≈ 10.8 ТБ

С учётом overhead векторного индекса (ANN, HNSW/IVF, ~1.2–2×):
13–22 ТБ для Vector Store + ANN.

---

#### Граф документов (k-NN)

Параметры:

- число соседей: k = 20;
- хранение одного ребра (ID + вес): ~16 байт.

Объём графа в день:

40 000 000 чанков × 20 × 16 байт ≈ 12.8 ГБ/день

Объём за 180 дней:

12.8 ГБ × 180 ≈ 2.3 ТБ

---

#### Итоговые требования к хранилищу

Raw Storage (180 дней): ~720 ТБ  
Эмбеддинги + ANN: ~13–22 ТБ  
Граф документов: ~2–3 ТБ  
Метаданные, ACL, логи: ~1–5 ТБ  

ИТОГО: ~740–760 ТБ

---

### 3.2 Пропускная способность и скорость эмбеддинга

Нагрузка:

- пиковая нагрузка: 16 994 RPS;
- критический этап: онлайн-эмбеддинг запроса.

Производительность BERT-подобных моделей (официальные источники NVIDIA):

- TensorRT позволяет снизить latency **BERT-Large** до **~1.2 мс на NVIDIA A100**
  в оптимизированной конфигурации  
  ([NVIDIA: Real-Time NLP with BERT using TensorRT](https://developer.nvidia.com/blog/real-time-nlp-with-bert-using-tensorrt-updated/),  
  [NVIDIA: TensorRT 8 Slashes BERT-Large Inference to 1 ms](https://developer.nvidia.com/blog/nvidia-announces-tensorrt-8-slashing-bert-large-inference-down-to-1-millisecond/)).

- Документация **Triton Inference Server** показывает, что на **NVIDIA T4**
  latency BERT-моделей существенно выше, а throughput составляет
  **десятки–сотни инференсов в секунду**, тогда как CPU-инференс измеряется секундами  
  ([NVIDIA Triton — NLP Benchmarks](https://docs.nvidia.com/ai-enterprise/deployment/natural-language-processing/latest/benchmark.html)).


Для продакшн-SLA закладывается консервативный бюджет:

- latency эмбеддинга (p95): 10–40 мс на GPU
  (с учётом batching и очередей).

---

#### Расчёт количества GPU

Консервативная оценка производительности одной GPU-реплики
(Query Embedding Service, A100/H100):

~2 000 запросов/сек.

Требуемое количество GPU:

16 994 RPS ÷ 2 000 RPS ≈ 8.5 → 9 GPU

С учётом отказоустойчивости и headroom:

12–16 GPU класса A100 / H100.

---

### 3.3 Проверка требования latency ≤ 417 мс

Оценка end-to-end задержки (p95):

API Gateway + сеть: 30–60 мс  
Query Embedding (GPU): 10–40 мс  
ANN retrieval: 10–30 мс  
Расширение по графу: 10–40 мс  
ACL / RBAC: 20–80 мс  
Ранжирование + сбор метаданных: 20–60 мс  
Формирование ответа: 10–30 мс  

Суммарно: ~340 мс

Запас по SLA:

417 мс − 340 мс ≈ 77 мс

---

### 3.4 Масштабируемость и надёжность

Масштабируемость:
- Stateless-сервисы (Search API, Query Embedding) масштабируются горизонтально (HPA).
- Vector Index шардируется и реплицируется.
- При росте нагрузки:
  - добавляются GPU;
  - используется более лёгкая модель эмбеддингов (MiniLM, E5-small);
  - применяется INT8 и агрессивное кэширование.

Надёжность:
- Репликация сервисов (N+1 / N+2).
- Liveness/Readiness probes.
- Регулярные бэкапы метаданных и индексов.
- Полное восстановление индексов из Raw Storage.
- Мониторинг p50/p95/p99 latency, RPS, error rate, cache hit rate.



## Список использованных источников

1. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019).  
   **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.**  
   Proceedings of NAACL-HLT.  
   https://arxiv.org/abs/1810.04805

2. Reimers, N., & Gurevych, I. (2019).  
   **Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.**  
   Proceedings of EMNLP.  
   https://arxiv.org/abs/1908.10084

3. Johnson, J., Douze, M., & Jégou, H. (2019).  
   **Billion-scale similarity search with GPUs.**  
   IEEE Transactions on Big Data.  
   https://arxiv.org/abs/1702.08734

4. Malkov, Y. A., & Yashunin, D. A. (2018).  
   **Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs (HNSW).**  
   IEEE Transactions on Pattern Analysis and Machine Intelligence.  
   https://arxiv.org/abs/1603.09320

5. Liu, T.-Y. (2009).  
   **Learning to Rank for Information Retrieval.**  
   Foundations and Trends in Information Retrieval.  
   https://www.nowpublishers.com/article/Details/INR-016

6. NVIDIA Corporation.  
   **Real-Time NLP with BERT using TensorRT.**  
   https://developer.nvidia.com/blog/real-time-nlp-with-bert-using-tensorrt-updated/

7. NVIDIA Corporation.  
   **TensorRT 8 Slashes BERT-Large Inference Down to 1 Millisecond.**  
   https://developer.nvidia.com/blog/nvidia-announces-tensorrt-8-slashing-bert-large-inference-down-to-1-millisecond/

8. NVIDIA Corporation.  
   **Triton Inference Server – NLP Benchmarks.**  
   https://docs.nvidia.com/ai-enterprise/deployment/natural-language-processing/latest/benchmark.html


9. Burns, B., Grant, B., Oppenheimer, D., Brewer, E., & Wilkes, J. (2016).  
    **Borg, Omega, and Kubernetes.**  
    ACM Queue.  
    https://queue.acm.org/detail.cfm?id=2898444

10. Kubernetes Documentation.  
    https://kubernetes.io/docs/

11. Apache Kafka Documentation.  
    https://kafka.apache.org/documentation/
